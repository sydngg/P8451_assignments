---
title: "Class 9: Sample Code"
author: "JAS"
date: "3/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Demonstrate Interaction using Regression Models and Tree-based Methods using Exposome Data from HELIX

### Load .Rdata file and merge into single data frame

Reminder: Merging into a single data frame is optional. Depends upon how you program. This example will assume you've merged everything into a single data frame.

```{r dataprep}
library(tidyverse)
library(caret)
library(rpart.plot)

#Load data using path of where file is stored
load("C:/Users/js5406/Downloads/exposome.RData")

#Merge all data frames into a single data frame. FYI, this is just a shortcut by combining baseR with piping from tidyverse. There are other ways of merging across three data frames that are likely more elegant.

studydata<-merge(exposome,phenotype,by="ID") %>% merge(covariates, by="ID")

#Strip off ID Variable
studydata$ID<-NULL
#Partition data for use in demonstration
train.indices<-createDataPartition(y=studydata$e3_bw,p=0.7,list=FALSE)
train.data<-studydata[train.indices, ]
test.data<-studydata[-train.indices, ]
```

### Create Models to examine whether two features interact using linear regression

Note I'm not scaling before running my glm models. If this were a prediction question, I would likely scale so that my coefficients would be interpretable for variable importance. But this is just to show how one codes interaction terms in R using glm. Would be similar if you used within the caret framework. I'm also showing how you would code interaction terms within an elastic net framework using caret.

You can replace the features here with features from your own research question if you'd like to being exploring interactions using linear regression.

```{r}
#Model 1: Three features, indoor NO2, building density and walkability metric, in relation to child birthweight (I'm assuming measures are consistent pre and postnatal. Likely a bad assumption but just for illustrative purposes)

model.1<-glm(e3_bw~h_NO2_Log+h_builtdens300_preg_Sqrt+h_walkability_mean_preg_None, data=train.data) 
summary(model.1)

#Model 2a: Including an interaction term between two features
model.2a<-glm(e3_bw~h_NO2_Log+h_builtdens300_preg_Sqrt+h_walkability_mean_preg_None+h_NO2_Log*h_builtdens300_preg_Sqrt, data=train.data)
summary(model.2a)

#Model 2b: Including all combinations of two-way interactions using shortcut
model.2b<-glm(e3_bw~(h_NO2_Log+h_builtdens300_preg_Sqrt+h_NO2_Log+h_walkability_mean_preg_None)^2, data=train.data)
summary(model.2b)

#Model 3: Using the caret framework to run an elastic-net with interaction terms between features

set.seed(100)

model.3<- train(
  e3_bw ~(h_NO2_Log+h_builtdens300_preg_Sqrt+h_NO2_Log+h_walkability_mean_preg_None)^2, data = train.data, preProcess="scale", method = "glmnet",
  trControl = trainControl("cv", number = 5),
 tuneLength=10
  )
#Print the values of alpha and lambda that gave best prediction
model.3$bestTune

coef(model.3$finalModel, model.3$bestTune$lambda)

model.3.pred <- model.3 %>% predict(test.data)

# Model prediction performance
data.frame(
  RMSE = RMSE(model.3.pred, test.data$e3_bw),
  Rsquare = R2(model.3.pred, test.data$e3_bw)
)

```

### Implement Random forest first then tree

Note, I'm not showing any other aspects of the pipeline. In the real-world, I would think about whether my findings replicate in holdout data. 

Here, I'm demonstrating using all of the exposome features. Please note this is completely inappropriate as some features are measured after birth, there are correlations between features, etc. This is *just* to demonstrate the code.


```{r}
#Creating new dataframe that only includes exposome and birthweight
set.seed(100)
studydata.2<-merge(exposome,phenotype[,1:2],by="ID") 
studydata.2$ID<-NULL

set.seed(100)

#Note, I'm only running 2 folds for time. Typically, this is 5 or 10.
train.control.rf<-trainControl(method="cv", number=2)

rf.output<-train(e3_bw ~., data=studydata.2, method="rf",trControl=train.control.rf, ntree=100)

rf.output$results

varImp(rf.output)

#Extract names of variables for importance, ordering by overall importance with largest at top
col_index <- varImp(rf.output)$importance %>% 
  mutate(names=row.names(.)) %>%
  arrange(-Overall)

#Restrict to just top 20 and add in outcome variable
tree.vars<-col_index$names[1:20]
tree.vars[21]<-"e3_bw"

#Subset data to include only the top 20 most important features and outcome
tree.data<-studydata.2[,tree.vars]

#Fit tree to data, using a set cp for demonstration purposes
tree.exposome<-train(e3_bw~., data=tree.data, method="rpart",trControl=trainControl(method="cv", number = 5), tuneGrid=expand.grid(cp=0.01))

tree.exposome
rpart.plot(tree.exposome$finalModel)
```


