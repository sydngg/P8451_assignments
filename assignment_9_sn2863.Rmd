---
title: 'Homework 9: Using real-world data for hypothesis generation'
author: "Sydney Ng UNI: sn2863"
date: "Due: Tuesday, March 23, 2021 at 5:30 PM"
output: 
  html_document: 
    toc: TRUE
    toc_float: TRUE
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Examining the features

### Load .Rdata file and merge into single data frame

```{r dataprep, message=FALSE}
library(tidyverse)
library(caret)
library(rpart.plot)

#Load data using path of where file is stored
load("./exposome.RData")

#Merge all data frames into a single data frame. FYI, this is just a shortcut by combining baseR with piping from tidyverse. There are other ways of merging across three data frames that are likely more elegant. Reminder: Merging into a single data frame is optional. Depends upon how you program. This example will assume you've merged everything into a single data frame.

studydata<-merge(exposome,phenotype,by="ID") %>% merge(covariates, by="ID") %>% 
  mutate(hs_asthma = as.factor(hs_asthma))

#Strip off ID Variable
studydata$ID<-NULL
```

### Looking at demographics and maternal information during pregnancy.

```{r}
summary(covariates)
```

* There are no missing values in the `covariates` dataset containing demographic and maternal information during the mother's pregnancy. We also see that there are 1301 mothers in this dataset.

* `h_parity_None`: Looking further at the mother's information, `r round(601/sum(601,464,236)*100,2)`% of the women were nulliparous, meaning they have never given birth before, `r round(464/sum(601,464,236)*100,2)`% have given birth once before, and `r round(236/sum(601,464,236)*100,2)`% have given birth at least twice before their current pregnancy.

* `h_edumc_None`: Looking further at the education of the mothers, `r round(178/sum(178,449,674)*100,2)`% have a primary school education, `r round(449/sum(178,449,674)*100,2)`% have a secondary school education, and  `r round(674/sum(178,449,674)*100,2)`% have a university degree or higher.


### Looking at health outcomes measured during the study.

```{r}
phenotype <-
  phenotype %>%
  mutate(hs_asthma = as.factor(hs_asthma))
summary(phenotype)

hist(phenotype$hs_zbmi_who)
```

* `hs_asthma`: `r round(1159/sum(1159, 142)*100,2)`% of children in the study have never had asthma `r round(142/sum(1159, 142)*100,2)`%  of children do. There is a big imbalance in asthma diagnosis. This could be a potential outcome to focus in on!

* `hs_zbmi_who`: Looking at the standardized BMI variable, we see that it does not deviate too far from a normal distribution, though can be slightly right skewed because there may be more obese children in the dataset.

* `hs_Gen_Tot`: Neuro behavior defined as internalizing and externalizing problems at 6-11 years old on the CBCL scale has a min of 0.00 and max of 133.00 and a median of 20.00. This also indicates that there is a heavy right skew in the distribution.

### Looking at the environmental features measured on children. 

```{r}
exposome %>% select(contains("pcb"), hs_pet_cat_r2_None, hs_pet_dog_r2_None) %>% summary()

hist(exposome$hs_sumPCBs5_cadj_Log2)
hist(exposome$hs_sumPCBs5_madj_Log2)


```

* I was particularly interested in looking at the concentrations of PCBs (Polychlorinated biphenyls) in the mothers and children. I very vaguely recall from Environmental DOH class that these industrial compounds only accumulate in our fatty tissues as well as pose a very toxic chemical risk and cause cancer.

* The variables that contain `cadj` are related to children and the variables containing `madj` are related to mothers. All of the measures summarized in this dataset seem to have been adjusted for lipids (the fats in our bodies) and log-2 transformed.

* `hs_sumPCBs6_cadj_Log2`: the sum of PCBs in children (log-2 transformed) has a mean of 4.647, median of 4.612, and IQR 3.857-5.372. After examining the histogram, there seem to be outliers in the long right tail of the distribution. It would be interesting to look at some of the other features that can predict these high toxic concentrations in children, to generate a hypothesis.

* `hs_sumPCBs6_madj_Log2`: the sum of PCBs in mothers (log-2 transformed) has a mean of 4.860, median of 4.715, and IQR 4.007-5.738. The concentrations of PCBs in mothers is comparable to that in children.

* `hs_pet_cat_r2_None`: `r round(1059/sum(1059, 242)*100,2)`% of women with children in the dataset do not have a cat at home, whereas `r round(242/sum(1059, 242)*100,2)`% do.

* `hs_pet_dog_r2_None`: `r round(1108/sum(1108, 193)*100,2)`% of women with children in the dataset do not have a cat at home, whereas `r round(193/sum(1108, 193)*100,2)`% do.


# Developing a Research Question

**Outcome:**  `hs_asthma` a binary variable, whether or not the child was ever diagnosed with asthma by a doctor

**Research Question:** What features from the exposome HELIX dataset can predict child asthma diagnosis?


# Implementing a Pipeline and Algorithm

### Looking at missingness and correlated variables, scaling and centering

```{r, message=FALSE, warning=FALSE}
library(Amelia)
missmap(studydata, main = "Missing values vs observed")

# Correlations
dat_num <- studydata %>% select(where(is.numeric))
dat_not_num <- studydata %>% select(!where(is.numeric))

correlations <- cor(dat_num, use="complete.obs")
high_correlations <- findCorrelation(correlations, cutoff=0.4) 

dat_num <- dat_num[-high_correlations] # getting rid of highly correlated columns

# Centering and Scaling
set.up.preprocess <- preProcess(dat_num, method = c("center", "scale"))
# Output pre-processed values
transformed_vals <- predict(set.up.preprocess, dat_num)
dat_transformed <- bind_cols(transformed_vals, dat_not_num)

# Checking the new dimensions of our dataset
dim(dat_transformed)
```

* After removing the highly correlated variables from our dataset, we have 146 variables remaining to work with. 

* Because of the high number of features, I will perform an elastic net (regularized) logistic regression for classification. Not only can this potentially reduce our feature space further, but it would also be of interest to look at which features are most important and have clinical significance in predicting asthma diagnosis in children. Interpretation could be useful and important.

### Partitioning the data
```{r}
# Partitioning data
set.seed(100)
train.indices<-createDataPartition(y=studydata$hs_asthma,p=0.7,list=FALSE)
training <-studydata[train.indices, ]
testing <-studydata[-train.indices, ]

# Checking distribution of the outcome
training %>% group_by(hs_asthma) %>% 
  summarise(count = n(), proportion = count/nrow(training))
testing %>% group_by(hs_asthma) %>% 
  summarise(count = n(), proportion = count/nrow(testing))
```

### Regularized (elastic net) logistic regression

```{r lasso}
set.seed(100)
lambda <- 10^seq(-3,3, length=100)
alpha <- seq(0.001,1, length=20)
train_control <- trainControl(method="cv", number = 10, sampling = "down")
logistic_reg <- train(hs_asthma ~., data = training, 
                      method = "glmnet", family = "binomial",
                      trControl = train_control, tuneLength=10,
                      tuneGrid=expand.grid(alpha=alpha, lambda=lambda))
logistic_reg$bestTune
```


* Down-sampling is used to address the outcome imbalance in our data. After performing a logistic regression with elastic net regularization, it turns out that the best tuning parameters for alpha and lambda are `r logistic_reg$bestTune[1]` and `r logistic_reg$bestTune[2]`, respectively.

### Elastic net logistic regression results and predictions on training data

```{r logistic_results}
# Coefficients for the regularized logistic regression
coef(logistic_reg$finalModel, logistic_reg$bestTune$lambda)

# Making predictions
logistic_pred <- logistic_reg %>% predict(training)

# Confusion Matrix
confusion_logistic <- 
  confusionMatrix(logistic_pred, training$hs_asthma,
                  positive = "1")
confusion_logistic$table
```

* After applying this elastic net regularized logistic regression model to the training dataset, we find that it is `r round(confusion_logistic$overall[1]*100, 2)`% accurate in predicting asthma diagnosis status, with a sensitivity and specificity of `r round(confusion_logistic$byClass[1]*100, 2)`% and `r round(confusion_logistic$byClass[2]*100, 2)`%, respectively. The prediction accuracy of this model is pretty mediocre, however, it correctly identifies those children who truly have asthma (i.e., 100% sensitivity).

### Elastic net logistic regression results and predictions on testing data.

```{r final_model_testing}
# Making predictions
pred_asthma_test <- predict(logistic_reg, testing)
pred_asthma_prob_test <- predict(logistic_reg, testing, type="prob")

# Printing the confusion matrix
results_test <- confusionMatrix(pred_asthma_test, testing$hs_asthma, positive = "1")
print(results_test)
```

